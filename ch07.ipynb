{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100bdef0",
   "metadata": {},
   "source": [
    "# 问答模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ead08",
   "metadata": {},
   "source": [
    "##  Step1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0078b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import random \n",
    "import math\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_dataset , Dataset , load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e865e7",
   "metadata": {},
   "source": [
    "## Step2 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e2cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "webQA_train_files = [r'G:\\Anaconda\\Kaggle\\gpt2\\data\\webQA\\train.json' , r'G:\\Anaconda\\Kaggle\\gpt2\\data\\webQA\\dev.json']\n",
    "du_train_files = [r'G:\\Anaconda\\Kaggle\\gpt2\\data\\dureader\\train.csv' , r'G:\\Anaconda\\Kaggle\\gpt2\\data\\dureader\\dev.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cf1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "webQA_train = load_dataset('json', data_files=webQA_train_files , split='train')\n",
    "du_train = load_dataset('csv' ,data_files= du_train_files , split = 'train')\n",
    "ds = Dataset.load_from_disk(r\"\\Anaconda\\Kaggle\\LLm\\transformers-code-master\\02-NLP Tasks\\16-generative_chatbot\\alpaca_data_zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5805d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '世界上最早的报纸诞生于',\n",
       " 'output': '中国。北宋末年(公元11,12世纪)出现的印刷报纸,不仅是中国新闻史上最早的印刷报纸,也是世界新闻史上最早的印刷报纸.中国新闻事业历史的悠久,内容的丰富,是任何西方国家都难以比肩的.<e>中国古代的报纸产生于中国的封建社会时期,是封建地主阶级及其政治代表占统治地位的封建自然经济通过新闻手段的反映.在漫长的封建社会时期,中国古代的报纸,不论是官方的邸报,还是民办的小报和京报,都必然要和当时的封建统治者保持一定的联系,受他们的制约.官方的邸报固然是封建统治阶级的喉舌和御用的宣传工具,民办的小报和京报也只能在封建统治阶级的控制下活动,不能越雷池一步.封建统治者绝不允许可以自由报道一切消息和自由发表一切意见的报纸存在.中国古代的报纸在为当时的读者提供朝野政治和社会信息方面确实起过一定的作用,但始终没有摆脱统治阶级的掌握.中国古代报纸的历史,基本上是一部封建统治阶级掌握传播媒介,控制舆论工具,限制言论出版自由的历史.<e>中国古代的邸报有1200年左右的历史.小报有近千年的历史.民间报房出版的邸报,京报有近400年的历史.它们从诞生到结束,持续的时间都不算短,但发展不快,形式内容的变化不大.',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webQA_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1f11a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': '第35集[SEP]第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。',\n",
       " 'text2': '仙剑奇侠传3第几集上天界'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "du_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be32f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func4ds(example):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh')\n",
    "    MAX_LENGTH = 512\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"提问: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\n回答: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ad96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func4web(example):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh')\n",
    "    MAX_LENGTH = 512\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join(\"提问: \" + example[\"input\"]).strip() + \"\\n\\n回答: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7c9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func4du(example):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh')\n",
    "    MAX_LENGTH = 512\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join(\"提问: \" + example[\"text2\"]).strip() + \"\\n\\n回答: \")\n",
    "    response = tokenizer(example[\"text1\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f4dd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325c65da98d447f88371c84091fae58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/26858 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func4ds ,remove_columns=ds.column_names,num_proc = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8028e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47f8e27d2f148b0ae335f3a00cbd659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/39192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_web = webQA_train.map(process_func4web ,remove_columns=webQA_train.column_names,num_proc = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb62c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e92720d1f048c3a669ac085a337b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/15937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_du = du_train.map(process_func4du , remove_columns=du_train.column_names , num_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "023a53b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87e047dc34c46029bf8ca787ef85cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/81987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged = concatenate_datasets([tokenized_ds, tokenized_web, tokenized_du])\n",
    "merged = merged.shuffle(seed=42)\n",
    "\n",
    "out_dir = r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\tokenized_merged\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "merged.save_to_disk(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef53b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da06180661664e83b9e5ff0186747d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/81987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_total: 10035888\n"
     ]
    }
   ],
   "source": [
    "def _len_batch(batch):\n",
    "    return {\"len\": [len(x) for x in batch[\"input_ids\"]]}\n",
    "\n",
    "lens = merged.map(_len_batch, batched=True, batch_size=1000, num_proc=6, remove_columns=[])\n",
    "total_tokens = int(sum(lens[\"len\"]))\n",
    "print(\"tokens_total:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f423bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a7cb1bced447d49b0ed460855eceb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/81987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_valid: 7369978\n"
     ]
    }
   ],
   "source": [
    "if \"labels\" in merged.column_names:\n",
    "    def _valid_batch(batch):\n",
    "        return {\"valid\": [sum(1 for t in x if t != -100) for x in batch[\"labels\"]]}\n",
    "    vlen = merged.map(_valid_batch, batched=True, batch_size=1000, num_proc=6, remove_columns=[])\n",
    "    total_valid = int(sum(vlen[\"valid\"]))\n",
    "    print(\"tokens_valid:\", total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0632ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = load_from_disk(r'G:\\Anaconda\\Kaggle\\gpt2\\data\\tokenized_merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73025bcd",
   "metadata": {},
   "source": [
    "## Step3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29df99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTModel\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        import os\n",
    "        import glob\n",
    "        import datasets\n",
    "        self._pylist = None\n",
    "        try:\n",
    "            ds = datasets.load_from_disk(data_dir)\n",
    "            if isinstance(ds, datasets.DatasetDict):\n",
    "                ds = ds.get(\"train\", list(ds.values())[0])\n",
    "            self.ds = ds\n",
    "        except Exception:\n",
    "            pattern = os.path.join(data_dir, \"data-*.arrow\")\n",
    "            files = sorted(glob.glob(pattern))\n",
    "            if not files:\n",
    "                raise\n",
    "            try:\n",
    "                ds = datasets.Dataset.from_file(files[0])\n",
    "                self.ds = ds\n",
    "            except Exception:\n",
    "                import pyarrow.ipc as pa_ipc\n",
    "                try:\n",
    "                    reader = pa_ipc.open_file(files[0])\n",
    "                    table = reader.read_all()\n",
    "                except Exception:\n",
    "                    reader = pa_ipc.open_stream(files[0])\n",
    "                    table = reader.read_all()\n",
    "                col = table.column(\"input_ids\")\n",
    "                self._pylist = col.to_pylist()\n",
    "                self.ds = None\n",
    "    def __len__(self):\n",
    "        return len(self._pylist) if self._pylist is not None else len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        if self._pylist is not None:\n",
    "            return {\"input_ids\": self._pylist[idx]}\n",
    "        ex = self.ds[idx]\n",
    "        return {\"input_ids\": ex[\"input_ids\"]}\n",
    "\n",
    "def load_tokenizer(tokenizer_dir):\n",
    "    import os\n",
    "    from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    except Exception:\n",
    "        vocab = os.path.join(tokenizer_dir, \"vocab.json\")\n",
    "        merges = os.path.join(tokenizer_dir, \"merges.txt\")\n",
    "        tok = GPT2TokenizerFast(vocab_file=vocab, merges_file=merges)\n",
    "    if tok.pad_token_id is None:\n",
    "        try:\n",
    "            tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return tok\n",
    "\n",
    "class LMDataCollator:\n",
    "    def __init__(self, context_length, pad_id):\n",
    "        self.context_length = context_length\n",
    "        self.pad_id = pad_id\n",
    "    def __call__(self, batch):\n",
    "        ids = []\n",
    "        for item in batch:\n",
    "            x = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "            if x.shape[0] >= self.context_length:\n",
    "                x = x[: self.context_length]\n",
    "            else:\n",
    "                pad = torch.full((self.context_length - x.shape[0],), self.pad_id, dtype=torch.long)\n",
    "                x = torch.cat([x, pad], dim=0)\n",
    "            ids.append(x)\n",
    "        input_ids = torch.stack(ids, dim=0)\n",
    "        attention_mask = (input_ids != self.pad_id).long()\n",
    "        labels = input_ids.roll(-1, dims=1)\n",
    "        labels[:, -1] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def build_lm_dataloader(data_dir, tokenizer_dir, batch_size, context_length, shuffle=True, num_workers=0):\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    pad_id = tok.pad_token_id if tok.pad_token_id is not None else 0\n",
    "    ds = LMDataset(data_dir)\n",
    "    collate = LMDataCollator(context_length, pad_id)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate)\n",
    "\n",
    "def get_vocab_size(tokenizer_dir):\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    try:\n",
    "        return int(len(tok))\n",
    "    except Exception:\n",
    "        return int(tok.vocab_size)\n",
    "\n",
    "def compute_lm_loss(logits, labels):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdcf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PackedBinDataset(Dataset):\n",
    "    def __init__(self, prefix):\n",
    "        # .idx 是二进制索引：int64 offset_tokens, int32 length_tokens\n",
    "        self.bin_path = prefix + \".bin\"\n",
    "        self.idx_path = prefix + \".idx\"\n",
    "        self.entries = []\n",
    "        with open(self.idx_path, \"rb\") as f:\n",
    "            while True:\n",
    "                rec = f.read(12)\n",
    "                if not rec:\n",
    "                    break\n",
    "                off = int.from_bytes(rec[:8], \"little\", signed=True)\n",
    "                ln  = int.from_bytes(rec[8:], \"little\", signed=True)\n",
    "                self.entries.append((off, ln))\n",
    "        # 用 memmap 读 .bin（int32）\n",
    "        self.mm = np.memmap(self.bin_path, dtype=np.int32, mode=\"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        off, ln = self.entries[idx]\n",
    "        arr = self.mm[off: off + ln]          # shape: (ctx_len,)\n",
    "        input_ids = torch.from_numpy(arr.astype(np.int64))   # to int64 tensor\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "def collate_packed(batch):\n",
    "    # 固定 ctx_len，无 padding。attention_mask 全 1；labels 右移一位，最后一位 -100\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    labels = input_ids.roll(-1, dims=1)\n",
    "    labels[:, -1] = -100\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcfd46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "from model import GPTModel, compute_lm_loss, get_vocab_size, load_tokenizer\n",
    "\n",
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from model import GPTModel, compute_lm_loss, get_vocab_size, load_tokenizer\n",
    "\n",
    "class PackedBinDataset(Dataset):\n",
    "    def __init__(self, prefix):\n",
    "        self.bin_path = prefix + \".bin\"\n",
    "        self.idx_path = prefix + \".idx\"\n",
    "        self.entries = []\n",
    "        with open(self.idx_path, \"rb\") as f:\n",
    "            while True:\n",
    "                rec = f.read(12)\n",
    "                if not rec: break\n",
    "                off = int.from_bytes(rec[:8], \"little\", signed=True)\n",
    "                ln  = int.from_bytes(rec[8:], \"little\", signed=True)\n",
    "                self.entries.append((off, ln))\n",
    "        self.mm = np.memmap(self.bin_path, dtype=np.int32, mode=\"r\")\n",
    "    def __len__(self): return len(self.entries)\n",
    "    def __getitem__(self, idx):\n",
    "        off, ln = self.entries[idx]\n",
    "        arr = self.mm[off: off + ln]\n",
    "        return {\"input_ids\": torch.from_numpy(arr.astype(np.int64))}\n",
    "\n",
    "def collate_packed(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    labels = input_ids.roll(-1, dims=1)\n",
    "    labels[:, -1] = -100\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "import os, math, torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.auto import tqdm\n",
    "from model import build_lm_dataloader, GPTModel, compute_lm_loss, get_vocab_size\n",
    "\n",
    "def notebook_train_hfds(\n",
    "    data_dir,\n",
    "    tokenizer_dir,\n",
    "    ctx_len=1024,\n",
    "    emb_dim=768, n_heads=12, n_layers=12, dropout=0.1, qkv_bias=False, tie_weights=True,\n",
    "    batch_size=8, num_workers=0,\n",
    "    lr=3e-4, weight_decay=0.1, warmup_steps=3000, max_steps=0, epochs=1, grad_accum=1,\n",
    "    log_every=50, save_every=1000, save_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\", resume=\"\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dl = build_lm_dataloader(data_dir, tokenizer_dir, batch_size, ctx_len, shuffle=True, num_workers=num_workers)\n",
    "    b = next(iter(dl))\n",
    "    max_id = int(b[\"input_ids\"].max().item())\n",
    "    vocab_nominal = get_vocab_size(tokenizer_dir)\n",
    "    vocab_eff = max(vocab_nominal, max_id + 1)\n",
    "    cfg = {\n",
    "        \"vocab_size\": vocab_eff,\n",
    "        \"context_length\": ctx_len,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"dropout\": dropout,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"qkv_bias\": qkv_bias,\n",
    "        \"n_layers\": n_layers,\n",
    "    }\n",
    "    model = GPTModel(cfg).to(device)\n",
    "    if tie_weights:\n",
    "        model.output_layer.weight = model.embedding_layer.embedding.weight\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if n.endswith(\"bias\") or (\"ln\" in n.lower()) or (\"layernorm\" in n.lower()):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    optim = AdamW([\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ], lr=lr)\n",
    "    total_steps = max_steps if (max_steps and max_steps > 0) else (epochs * len(dl))\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(1, warmup_steps)\n",
    "        return max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps))\n",
    "    scheduler = LambdaLR(optim, lr_lambda)\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler('cuda' if device == 'cuda' else 'cpu')\n",
    "    except Exception:\n",
    "        from torch.cuda.amp import GradScaler as _GS\n",
    "        scaler = _GS()\n",
    "    start_step = 0\n",
    "    if resume:\n",
    "        ckpt = torch.load(resume, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt.get(\"model\", {}), strict=False)\n",
    "        try:\n",
    "            optim.load_state_dict(ckpt.get(\"optimizer\", {}))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            scaler.load_state_dict(ckpt.get(\"scaler\", {}))\n",
    "        except Exception:\n",
    "            pass\n",
    "        start_step = int(ckpt.get(\"step\", 0))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.train()\n",
    "    step = start_step\n",
    "    running, count = 0.0, 0\n",
    "    p_epoch = tqdm(total=total_steps, desc=\"训练\", unit=\"step\")\n",
    "    for _ in range(epochs):\n",
    "        for batch in dl:\n",
    "            step += 1\n",
    "            if device == \"cuda\":\n",
    "                try:\n",
    "                    from torch.amp import autocast as _ac\n",
    "                    _cuda_ctx = _ac('cuda')\n",
    "                except Exception:\n",
    "                    from torch.cuda.amp import autocast as _ac\n",
    "                    _cuda_ctx = _ac()\n",
    "                with _cuda_ctx:\n",
    "                    logits = model(batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "                    loss = compute_lm_loss(logits, batch[\"labels\"].to(device)) / grad_accum\n",
    "            else:\n",
    "                logits = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "                loss = compute_lm_loss(logits, batch[\"labels\"]) / grad_accum\n",
    "            scaler.scale(loss).backward()\n",
    "            if step % grad_accum == 0:\n",
    "                scaler.unscale_(optim)\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "            running += loss.item()\n",
    "            count += 1\n",
    "            if log_every and step % log_every == 0:\n",
    "                avg = running * grad_accum / max(1, count)\n",
    "                ppl = math.exp(avg) if avg < 700 else float('inf')\n",
    "                lr_now = scheduler.get_last_lr()[0]\n",
    "                valid_tokens = int((batch[\"labels\"] != -100).sum().item())\n",
    "                p_epoch.set_postfix_str(f\"loss {avg:.4f} | ppl {ppl:.2f} | lr {lr_now:.6f} | valid {valid_tokens}\")\n",
    "                running, count = 0.0, 0\n",
    "            if save_every and step % save_every == 0:\n",
    "                path = os.path.join(save_dir, f\"ckpt_step_{step}.pt\")\n",
    "                _obj = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optim.state_dict(),\n",
    "                    \"scaler\": scaler.state_dict(),\n",
    "                    \"step\": step,\n",
    "                    \"cfg\": cfg,\n",
    "                }\n",
    "                try:\n",
    "                    torch.save(_obj, path)\n",
    "                except RuntimeError:\n",
    "                    torch.save(_obj, path, _use_new_zipfile_serialization=False)\n",
    "            p_epoch.update(1)\n",
    "            if step >= total_steps:\n",
    "                break\n",
    "    p_epoch.close()\n",
    "    return model\n",
    "\n",
    "def save_final_model(model, save_dir=r'G:\\Anaconda\\Kaggle\\gpt2\\checkpoints', file_name=\"final_model.pt\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    final_ckpt = os.path.join(save_dir, file_name)\n",
    "    sd_cpu = {}\n",
    "    for k, v in tqdm(model.state_dict().items(), desc=\"准备最终权重\", unit=\"param\"):\n",
    "        sd_cpu[k] = v.detach().cpu()\n",
    "    try:\n",
    "        torch.save({'model': sd_cpu}, final_ckpt)\n",
    "    except RuntimeError:\n",
    "        torch.save({'model': sd_cpu}, final_ckpt, _use_new_zipfile_serialization=False)\n",
    "    return final_ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b30f3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504e292075294b75ad7055a687b410cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练:   0%|          | 0/81987 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnotebook_train_hfds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtokenized_merged\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtokenizer_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbytebpe_zh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mctx_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_accum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfinal_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m final_path \u001b[38;5;241m=\u001b[39m save_final_model(model, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnaconda\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKaggle\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model_step3.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_path)\n",
      "Cell \u001b[1;32mIn[5], line 141\u001b[0m, in \u001b[0;36mnotebook_train_hfds\u001b[1;34m(data_dir, tokenizer_dir, ctx_len, emb_dim, n_heads, n_layers, dropout, qkv_bias, tie_weights, batch_size, num_workers, lr, weight_decay, warmup_steps, max_steps, epochs, grad_accum, log_every, save_every, save_dir, resume)\u001b[0m\n\u001b[0;32m    139\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    140\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_lm_loss(logits, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m grad_accum\n\u001b[1;32m--> 141\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m grad_accum \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optim)\n",
      "File \u001b[1;32mg:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Anaconda\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = notebook_train_hfds(\n",
    "  data_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\tokenized_merged\",\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  ctx_len=1024,\n",
    "  emb_dim=768, n_heads=12, n_layers=12, dropout=0.1, qkv_bias=False, tie_weights=True,\n",
    "  batch_size=1, num_workers=0,\n",
    "  lr=3e-4, weight_decay=0.1, warmup_steps=3000, max_steps=0, epochs=1, grad_accum=1,\n",
    "  log_every=50, save_every=1000,\n",
    "  save_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\",\n",
    "  resume=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\\final_model.pt\"\n",
    ")\n",
    "\n",
    "final_path = save_final_model(model, save_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\", file_name=\"final_model_step3.pt\")\n",
    "print(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5938d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国科技应该提供一个关于网络安全信息，并根据用户信息进行进行分析，以便用户信息进行分析。\n"
     ]
    }
   ],
   "source": [
    "from train_hfds import qa_generate\n",
    "\n",
    "answer = qa_generate(\n",
    "  question=\"提问：中国科技应该如何发展？ 回答：\",\n",
    "  ckpt_path=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\\ckpt_step_2000.pt\",   # 或 final_model.pt\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  max_new_tokens=512,\n",
    "  temperature=0.8,\n",
    "  top_k=30,\n",
    "  top_p=0.8,\n",
    "  device='cuda',\n",
    "  use_half=True,\n",
    "  ctx_len=1024, emb_dim=768, n_heads=12, n_layers=12, dropout=0.1, qkv_bias=False, tie_weights=True\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab87a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
