{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55065bcb",
   "metadata": {},
   "source": [
    "# 中文模型预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984865f",
   "metadata": {},
   "source": [
    "## Step1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caeb7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments , Trainer\n",
    "from tokenizers import  ByteLevelBPETokenizer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from tqdm.auto import tqdm\n",
    "import os, re\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import hashlib\n",
    "import random\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f317fc",
   "metadata": {},
   "source": [
    "## Step2 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c59dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "thu = load_dataset(\n",
    "    \"text\",\n",
    "    data_files=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\THUCNews1\\THUCNews1\\**\\*.txt\",\n",
    "    split=\"train\",\n",
    "    cache_dir= r'G:\\Anaconda\\Kaggle\\gpt2\\data',\n",
    "    num_proc = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thu = load_dataset(r'G:\\Anaconda\\Kaggle\\gpt2\\data\\text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0781bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def iter_wiki_pages(\n",
    "    xml_file,\n",
    "    min_text_length=200,\n",
    "    sample_ratio=0.3,\n",
    "    max_keep=None,\n",
    "    max_seen=None,\n",
    "    skip_non_main=True,\n",
    "    seed=42,\n",
    "):\n",
    "    try:\n",
    "        from lxml import etree as ET\n",
    "        use_lxml = True\n",
    "    except Exception:\n",
    "        import xml.etree.ElementTree as ET\n",
    "        use_lxml = False\n",
    "\n",
    "    import random\n",
    "    rnd = random.Random(seed)\n",
    "\n",
    "    context = ET.iterparse(xml_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    ns = ''\n",
    "    if '}' in root.tag:\n",
    "        ns = root.tag.split('}')[0] + '}'\n",
    "\n",
    "    kept = 0\n",
    "    seen = 0\n",
    "    pbar = tqdm(desc='解析页面', unit='page')\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag == f'{ns}page':\n",
    "            if max_seen is not None and seen >= max_seen:\n",
    "                break\n",
    "            seen += 1\n",
    "\n",
    "            title_elem = elem.find(f'{ns}title')\n",
    "            text_elem = elem.find(f'{ns}revision/{ns}text')\n",
    "            title = title_elem.text or '' if title_elem is not None else ''\n",
    "            text = text_elem.text or '' if text_elem is not None else ''\n",
    "\n",
    "            if skip_non_main and ':' in title:\n",
    "                pass\n",
    "            elif len(text) >= min_text_length:\n",
    "                if sample_ratio is None or rnd.random() < sample_ratio:\n",
    "                    yield {'title': title, 'text': text}\n",
    "                    kept += 1\n",
    "                    if max_keep is not None and kept >= max_keep:\n",
    "                        break\n",
    "\n",
    "            if use_lxml:\n",
    "                elem.clear()\n",
    "                while elem.getprevious() is not None:\n",
    "                    del elem.getparent()[0]\n",
    "            else:\n",
    "                root.clear()\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "def load_wiki_xml_as_dataset(xml_file, **kwargs):\n",
    "    features = datasets.Features({'title': datasets.Value('string'), 'text': datasets.Value('string')})\n",
    "    return datasets.Dataset.from_generator(iter_wiki_pages, gen_kwargs={'xml_file': xml_file, **kwargs}, features=features)\n",
    "\n",
    "wiki_xml_path = r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\zhwiki-20251120-pages-articles-multistream.xml\\zhwiki-20251120-pages-articles-multistream.xml\"\n",
    "\n",
    "wiki_ds_30 = load_wiki_xml_as_dataset(\n",
    "    wiki_xml_path,\n",
    "    min_text_length=200,\n",
    "    sample_ratio=0.3,\n",
    "    max_keep=None,    # 可设置成比如 300000，进一步上限控制\n",
    "    max_seen=None,    # 可设置成比如 1000000，只扫描前100万页\n",
    "    skip_non_main=True,\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0454da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_arrow = r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki_ds\"\n",
    "wiki_ds_30.save_to_disk(save_dir_arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_arrow = r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\thu_ds\"\n",
    "thu.save_to_disk(save_dir_arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aded645",
   "metadata": {},
   "outputs": [],
   "source": [
    "thu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ds_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c64372",
   "metadata": {},
   "outputs": [],
   "source": [
    "thu['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ds_30[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "thu = load_from_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\thu_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds = load_from_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_batch(batch):\n",
    "    import os, re\n",
    "    def make_cleaner():\n",
    "            comment = re.compile(r'<!--.*?-->', re.DOTALL)\n",
    "            ref_tag = re.compile(r'<ref[^>]*>.*?</ref>', re.DOTALL)\n",
    "            html_tag = re.compile(r'<[^>]+>')\n",
    "            template = re.compile(r'\\{\\{[^{}]*\\}\\}')\n",
    "            table = re.compile(r'\\{\\|[\\s\\S]*?\\|\\}', re.DOTALL)\n",
    "            external_link = re.compile(r'\\[https?:\\/\\/[^\\s\\]]+(?:\\s+([^\\]]+))?\\]')\n",
    "            internal_link = re.compile(r'\\[\\[([^|\\]]+)(?:\\|([^\\]]+))?\\]\\]')\n",
    "            category = re.compile(r'\\[\\[(?:Category|分类):[^\\]]+\\]\\]', re.IGNORECASE)\n",
    "            filelink = re.compile(r'\\[\\[(?:File|Image|文件|图像):[^\\]]+\\]\\]', re.IGNORECASE)\n",
    "            bolditalic = re.compile(r\"'''''(.*?)'''''\", re.DOTALL)\n",
    "            bold = re.compile(r\"'''(.*?)'''\", re.DOTALL)\n",
    "            italic = re.compile(r\"''(.*?)''\", re.DOTALL)\n",
    "            list_marks = re.compile(r'^[*#;:]+\\s*', re.MULTILINE)\n",
    "            heading = re.compile(r'(==+)\\s*(.*?)\\s*\\1')\n",
    "            spaces = re.compile(r'[ \\t]+')\n",
    "            blanklines = re.compile(r'\\n{3,}')\n",
    "            def clean_text(text):\n",
    "                text = comment.sub('', text)\n",
    "                text = ref_tag.sub('', text)\n",
    "                text = table.sub('', text)\n",
    "                for _ in range(6):\n",
    "                    new = template.sub('', text)\n",
    "                    if new == text:\n",
    "                        break\n",
    "                    text = new\n",
    "                text = external_link.sub(lambda m: m.group(1) or '', text)\n",
    "                text = internal_link.sub(lambda m: (m.group(2) or m.group(1)), text)\n",
    "                text = bolditalic.sub(r'\\1', text)\n",
    "                text = bold.sub(r'\\1', text)\n",
    "                text = italic.sub(r'\\1', text)\n",
    "                text = category.sub('', text)\n",
    "                text = filelink.sub('', text)\n",
    "                text = html_tag.sub('', text)\n",
    "                text = list_marks.sub('', text)\n",
    "                text = heading.sub(lambda m: m.group(2), text)\n",
    "                text = spaces.sub(' ', text)\n",
    "                text = re.sub(r' +\\n', '\\n', text)\n",
    "                text = blanklines.sub('\\n\\n', text)\n",
    "                return text.strip()\n",
    "            return clean_text\n",
    "    clean_one = make_cleaner()\n",
    "    return {'text': [clean_one(t) for t in batch['text']]}\n",
    "\n",
    "wiki_ds_30 = wiki_ds_30.select_columns(['text'])\n",
    "cleaned_ds = wiki_ds_30.map(\n",
    "    clean_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=max(1, os.cpu_count() // 2),\n",
    "    desc='清洗文本'\n",
    ")\n",
    "cleaned_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_arrow = r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki\"\n",
    "cleaned_ds.save_to_disk(save_dir_arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ecd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def _convert_batch(batch):\n",
    "    from opencc import OpenCC\n",
    "    _cc = OpenCC('t2s')\n",
    "    def _to_simplified(x):\n",
    "        return _cc.convert(x)\n",
    "    return {'text': [_to_simplified(t) for t in batch['text']]}\n",
    "\n",
    "cleaned_ds_simp = cleaned_ds.map(\n",
    "    _convert_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=max(1, os.cpu_count() // 2),\n",
    "    desc='繁转简'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds_simp.save_to_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki_simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60016ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds_simp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263155d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_batch(batch, _state={'rx': None, 'banned': None, 'en_heading': None}):\n",
    "    import os, re\n",
    "    if _state['rx'] is None:\n",
    "        rx = {}\n",
    "        rx[\"comment\"] = re.compile(r'<!--.*?-->', re.DOTALL)\n",
    "        rx[\"ref\"] = re.compile(r'<ref[^>]*>.*?</ref>', re.DOTALL)\n",
    "        rx[\"html\"] = re.compile(r'<[^>]+>')\n",
    "        rx[\"template\"] = re.compile(r'\\{\\{[^{}]*\\}\\}')\n",
    "        rx[\"table\"] = re.compile(r'\\{\\|[\\s\\S]*?\\|\\}', re.DOTALL)\n",
    "        rx[\"external\"] = re.compile(r'\\[https?:\\/\\/[^\\s\\]]+(?:\\s+([^\\]]+))?\\]')\n",
    "        rx[\"internal\"] = re.compile(r'\\[\\[([^|\\]]+)(?:\\|([^\\]]+))?\\]\\]')\n",
    "        rx[\"category\"] = re.compile(r'\\[\\[(?:Category|分类):[^\\]]+\\]\\]', re.IGNORECASE)\n",
    "        rx[\"filelink\"] = re.compile(r'\\[\\[(?:File|Image|文件|图像):[^\\]]+\\]\\]', re.IGNORECASE)\n",
    "        rx[\"bolditalic\"] = re.compile(r\"'''''(.*?)'''''\", re.DOTALL)\n",
    "        rx[\"bold\"] = re.compile(r\"'''(.*?)'''\", re.DOTALL)\n",
    "        rx[\"italic\"] = re.compile(r\"''(.*?)''\", re.DOTALL)\n",
    "        rx[\"listmarks\"] = re.compile(r'^[*#;:]+\\s*', re.MULTILINE)\n",
    "        rx[\"heading\"] = re.compile(r'(==+)\\s*(.*?)\\s*\\1')\n",
    "        rx[\"spaces\"] = re.compile(r'[ \\t]+')\n",
    "        rx[\"blanklines\"] = re.compile(r'\\n{3,}')\n",
    "        _state['rx'] = rx\n",
    "        _state['banned'] = {\n",
    "            '参考文献','参考资料','延伸阅读','外部链接','外部连结','参见','注解',\n",
    "            '参考','相关条目','外部资源','扩展阅读','入门','专题介绍','选集','参考著作'\n",
    "        }\n",
    "        _state['en_heading'] = {'References','External links','See also','Further reading','Overview','Introduction'}\n",
    "\n",
    "    rx = _state['rx']\n",
    "    banned = _state['banned']\n",
    "    en_heading = _state['en_heading']\n",
    "\n",
    "    def ascii_ratio(s):\n",
    "        a = sum(1 for ch in s if ('A' <= ch <= 'Z') or ('a' <= ch <= 'z'))\n",
    "        return a / max(1, len(s))\n",
    "\n",
    "    def remove_ascii_parens(text):\n",
    "        def repl_cn(m):\n",
    "            inner = m.group(1)\n",
    "            return '' if ascii_ratio(inner) >= 0.4 else m.group(0)\n",
    "        def repl_en(m):\n",
    "            inner = m.group(1)\n",
    "            return '' if ascii_ratio(inner) >= 0.4 else m.group(0)\n",
    "        text = re.sub(r'（([^）]+)）', repl_cn, text)\n",
    "        text = re.sub(r'\\(([^\\)]+)\\)', repl_en, text)\n",
    "        return text\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = rx[\"comment\"].sub('', text)\n",
    "        text = rx[\"ref\"].sub('', text)\n",
    "        text = rx[\"table\"].sub('', text)\n",
    "        for _ in range(6):\n",
    "            new = rx[\"template\"].sub('', text)\n",
    "            if new == text:\n",
    "                break\n",
    "            text = new\n",
    "        text = re.sub(r'\\}\\}+', '', text)\n",
    "        text = rx[\"external\"].sub(lambda m: m.group(1) or '', text)\n",
    "        text = rx[\"internal\"].sub(lambda m: (m.group(2) or m.group(1)), text)\n",
    "        text = rx[\"bolditalic\"].sub(r'\\1', text)\n",
    "        text = rx[\"bold\"].sub(r'\\1', text)\n",
    "        text = rx[\"italic\"].sub(r'\\1', text)\n",
    "        text = rx[\"category\"].sub('', text)\n",
    "        text = rx[\"filelink\"].sub('', text)\n",
    "        text = rx[\"html\"].sub('', text)\n",
    "        text = rx[\"listmarks\"].sub('', text)\n",
    "        text = rx[\"heading\"].sub(lambda m: m.group(2), text)\n",
    "        text = rx[\"spaces\"].sub(' ', text)\n",
    "        text = re.sub(r' +\\n', '\\n', text)\n",
    "        text = rx[\"blanklines\"].sub('\\n\\n', text)\n",
    "        text = remove_ascii_parens(text)\n",
    "        return text.strip()\n",
    "\n",
    "    def strip_noise(text):\n",
    "        out = []\n",
    "        drop_rest = False\n",
    "        for line in text.splitlines():\n",
    "            s = line.strip()\n",
    "            if s == '':\n",
    "                if not drop_rest:\n",
    "                    out.append('')\n",
    "                continue\n",
    "            if s in banned or s in en_heading:\n",
    "                drop_rest = True\n",
    "                break\n",
    "            if s.startswith('Category:') or s.startswith('分类:'):\n",
    "                continue\n",
    "            if 'ISBN' in s or 'ISSN' in s:\n",
    "                continue\n",
    "            if ascii_ratio(s) >= 0.4:\n",
    "                continue\n",
    "            out.append(line)\n",
    "        res = '\\n'.join(out)\n",
    "        res = rx[\"blanklines\"].sub('\\n\\n', res).strip()\n",
    "        return res\n",
    "\n",
    "    return {'text': [strip_noise(clean_text(t)) for t in batch['text']]}\n",
    "cleaned_ds_final = cleaned_ds_simp.map(\n",
    "    strip_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=max(1, os.cpu_count() // 2),\n",
    "    desc='二次清洗'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da85762",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds_final.save_to_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cadaa",
   "metadata": {},
   "source": [
    "### 如果已经处理并保存了可以直接读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thu = load_from_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\thu_ds\")\n",
    "wiki_ds = load_from_disk(r'G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51194c7",
   "metadata": {},
   "source": [
    "## Step3 训练tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "def iter_ds_text_with_progress(\n",
    "    ds,\n",
    "    sample_ratio=0.3,\n",
    "    seed=42,\n",
    "    max_examples=None,\n",
    "    text_column=\"text\",\n",
    "    min_len=50,\n",
    "    desc=\"遍历样本\"\n",
    "):\n",
    "    import random\n",
    "    rnd = random.Random(seed)\n",
    "    try:\n",
    "        total = len(ds)\n",
    "    except Exception:\n",
    "        total = None\n",
    "    pbar = tqdm(total=total, desc=desc, unit=\"样本\")\n",
    "    count = 0\n",
    "    for ex in ds:\n",
    "        if total is not None:\n",
    "            pbar.update(1)\n",
    "        if isinstance(ex, str):\n",
    "            t = ex\n",
    "        else:\n",
    "            t = ex.get(text_column, \"\") if isinstance(ex, dict) else \"\"\n",
    "        if not t:\n",
    "            continue\n",
    "        if sample_ratio is None or rnd.random() < sample_ratio:\n",
    "            for p in t.split(\"\\n\"):\n",
    "                p = p.strip()\n",
    "                if len(p) >= min_len:\n",
    "                    yield p\n",
    "                    count += 1\n",
    "                    if max_examples and count >= max_examples:\n",
    "                        pbar.close()\n",
    "                        return\n",
    "    pbar.close()\n",
    "\n",
    "wiki_iter = iter_ds_text_with_progress(\n",
    "    wiki_ds,\n",
    "    sample_ratio=0.3,\n",
    "    seed=42,\n",
    "    max_examples=1_000_000,\n",
    "    text_column=\"text\",\n",
    "    min_len=50,\n",
    "    desc=\"Wiki\"\n",
    ")\n",
    "\n",
    "thu_iter = iter_ds_text_with_progress(\n",
    "    thu,\n",
    "    sample_ratio=0.2,\n",
    "    seed=42,\n",
    "    max_examples=500_000,\n",
    "    text_column=\"text\",\n",
    "    min_len=50,\n",
    "    desc=\"THUCNews\"\n",
    ")\n",
    "\n",
    "def corpus_iter_with_tqdm():\n",
    "    pbar = tqdm(desc=\"训练语料段数\", unit=\"段\")\n",
    "    for t in wiki_iter:\n",
    "        pbar.update(1)\n",
    "        yield t\n",
    "    for t in thu_iter:\n",
    "        pbar.update(1)\n",
    "        yield t\n",
    "    pbar.close()\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "tokenizer.train_from_iterator(\n",
    "    corpus_iter_with_tqdm(),\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<|pad|>\", \"<|bos|>\", \"<|eos|>\", \"<|unk|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "tokenizer.save_model(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb89b9",
   "metadata": {},
   "source": [
    "## Step4 使用tokenizer处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_build import build_pack_from_arrow_buckets\n",
    "\n",
    "build_pack_from_arrow_buckets(\n",
    "  arrow_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\wiki_dataset\",\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  out_prefix=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\wiki_1b\",\n",
    "  ctx_len=1024,\n",
    "  target_tokens=1_000_000_000,\n",
    "  bucket_ratios=(0.15, 0.35, 0.35, 0.15),\n",
    "  seed=42,\n",
    "  min_chars=8,\n",
    "  max_doc_chars=8000,\n",
    "  batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_build import build_pack_from_arrow_buckets_streaming\n",
    "\n",
    "build_pack_from_arrow_buckets_streaming(\n",
    "  arrow_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\thu_ds\",\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  out_prefix=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\thu_1b\",\n",
    "  ctx_len=1024,\n",
    "  target_tokens=1_000_000_000,\n",
    "  bucket_ratios=(0.15, 0.35, 0.35, 0.15),\n",
    "  seed=42,\n",
    "  min_chars=8,\n",
    "  max_doc_chars=8000,\n",
    "  batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\wiki_1b.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4482177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_build import mix_packed_bins\n",
    "\n",
    "mix_packed_bins(\n",
    "  prefixes=[\n",
    "    r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\thu_1b\",\n",
    "    r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\wiki_1b\",\n",
    "  ],\n",
    "  out_prefix=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\mix_2b\",\n",
    "  shuffle=True,\n",
    "  seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e332f",
   "metadata": {},
   "source": [
    "## Step5 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70408922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b13a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        import os\n",
    "        import glob\n",
    "        import datasets\n",
    "        self._pylist = None\n",
    "        try:\n",
    "            ds = datasets.load_from_disk(data_dir)\n",
    "            if isinstance(ds, datasets.DatasetDict):\n",
    "                ds = ds.get(\"train\", list(ds.values())[0])\n",
    "            self.ds = ds\n",
    "        except Exception:\n",
    "            pattern = os.path.join(data_dir, \"data-*.arrow\")\n",
    "            files = sorted(glob.glob(pattern))\n",
    "            if not files:\n",
    "                raise\n",
    "            try:\n",
    "                ds = datasets.Dataset.from_file(files[0])\n",
    "                self.ds = ds\n",
    "            except Exception:\n",
    "                import pyarrow.ipc as pa_ipc\n",
    "                try:\n",
    "                    reader = pa_ipc.open_file(files[0])\n",
    "                    table = reader.read_all()\n",
    "                except Exception:\n",
    "                    reader = pa_ipc.open_stream(files[0])\n",
    "                    table = reader.read_all()\n",
    "                col = table.column(\"input_ids\")\n",
    "                self._pylist = col.to_pylist()\n",
    "                self.ds = None\n",
    "    def __len__(self):\n",
    "        return len(self._pylist) if self._pylist is not None else len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        if self._pylist is not None:\n",
    "            return {\"input_ids\": self._pylist[idx]}\n",
    "        ex = self.ds[idx]\n",
    "        return {\"input_ids\": ex[\"input_ids\"]}\n",
    "\n",
    "def load_tokenizer(tokenizer_dir):\n",
    "    import os\n",
    "    from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    except Exception:\n",
    "        vocab = os.path.join(tokenizer_dir, \"vocab.json\")\n",
    "        merges = os.path.join(tokenizer_dir, \"merges.txt\")\n",
    "        tok = GPT2TokenizerFast(vocab_file=vocab, merges_file=merges)\n",
    "    if tok.pad_token_id is None:\n",
    "        try:\n",
    "            tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return tok\n",
    "\n",
    "class LMDataCollator:\n",
    "    def __init__(self, context_length, pad_id):\n",
    "        self.context_length = context_length\n",
    "        self.pad_id = pad_id\n",
    "    def __call__(self, batch):\n",
    "        ids = []\n",
    "        for item in batch:\n",
    "            x = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "            if x.shape[0] >= self.context_length:\n",
    "                x = x[: self.context_length]\n",
    "            else:\n",
    "                pad = torch.full((self.context_length - x.shape[0],), self.pad_id, dtype=torch.long)\n",
    "                x = torch.cat([x, pad], dim=0)\n",
    "            ids.append(x)\n",
    "        input_ids = torch.stack(ids, dim=0)\n",
    "        attention_mask = (input_ids != self.pad_id).long()\n",
    "        labels = input_ids.roll(-1, dims=1)\n",
    "        labels[:, -1] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def build_lm_dataloader(data_dir, tokenizer_dir, batch_size, context_length, shuffle=True, num_workers=0):\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    pad_id = tok.pad_token_id if tok.pad_token_id is not None else 0\n",
    "    ds = LMDataset(data_dir)\n",
    "    collate = LMDataCollator(context_length, pad_id)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate)\n",
    "\n",
    "def get_vocab_size(tokenizer_dir):\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    try:\n",
    "        return int(len(tok))\n",
    "    except Exception:\n",
    "        return int(tok.vocab_size)\n",
    "\n",
    "def compute_lm_loss(logits, labels):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7980d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PackedBinDataset(Dataset):\n",
    "    def __init__(self, prefix):\n",
    "        # .idx 是二进制索引：int64 offset_tokens, int32 length_tokens\n",
    "        self.bin_path = prefix + \".bin\"\n",
    "        self.idx_path = prefix + \".idx\"\n",
    "        self.entries = []\n",
    "        with open(self.idx_path, \"rb\") as f:\n",
    "            while True:\n",
    "                rec = f.read(12)\n",
    "                if not rec:\n",
    "                    break\n",
    "                off = int.from_bytes(rec[:8], \"little\", signed=True)\n",
    "                ln  = int.from_bytes(rec[8:], \"little\", signed=True)\n",
    "                self.entries.append((off, ln))\n",
    "        # 用 memmap 读 .bin（int32）\n",
    "        self.mm = np.memmap(self.bin_path, dtype=np.int32, mode=\"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        off, ln = self.entries[idx]\n",
    "        arr = self.mm[off: off + ln]          # shape: (ctx_len,)\n",
    "        input_ids = torch.from_numpy(arr.astype(np.int64))   # to int64 tensor\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "def collate_packed(batch):\n",
    "    # 固定 ctx_len，无 padding。attention_mask 全 1；labels 右移一位，最后一位 -100\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    labels = input_ids.roll(-1, dims=1)\n",
    "    labels[:, -1] = -100\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e082f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "from model import GPTModel, compute_lm_loss, get_vocab_size, load_tokenizer\n",
    "\n",
    "import os, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from model import GPTModel, compute_lm_loss, get_vocab_size, load_tokenizer\n",
    "\n",
    "class PackedBinDataset(Dataset):\n",
    "    def __init__(self, prefix):\n",
    "        self.bin_path = prefix + \".bin\"\n",
    "        self.idx_path = prefix + \".idx\"\n",
    "        self.entries = []\n",
    "        with open(self.idx_path, \"rb\") as f:\n",
    "            while True:\n",
    "                rec = f.read(12)\n",
    "                if not rec: break\n",
    "                off = int.from_bytes(rec[:8], \"little\", signed=True)\n",
    "                ln  = int.from_bytes(rec[8:], \"little\", signed=True)\n",
    "                self.entries.append((off, ln))\n",
    "        self.mm = np.memmap(self.bin_path, dtype=np.int32, mode=\"r\")\n",
    "    def __len__(self): return len(self.entries)\n",
    "    def __getitem__(self, idx):\n",
    "        off, ln = self.entries[idx]\n",
    "        arr = self.mm[off: off + ln]\n",
    "        return {\"input_ids\": torch.from_numpy(arr.astype(np.int64))}\n",
    "\n",
    "def collate_packed(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    labels = input_ids.roll(-1, dims=1)\n",
    "    labels[:, -1] = -100\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "def notebook_train(\n",
    "    bin_prefix,\n",
    "    tokenizer_dir,\n",
    "    ctx_len=1024,\n",
    "    emb_dim=768, n_heads=12, n_layers=12, dropout=0.1, qkv_bias=True, tie_weights=True,\n",
    "    batch_size=8, num_workers=0,\n",
    "    lr=3e-4, weight_decay=0.1, warmup_steps=3000, max_steps=0, epochs=1, grad_accum=1,\n",
    "    log_every=50, save_every=1000, save_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\", resume=\"\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    vocab_nominal = get_vocab_size(tokenizer_dir)\n",
    "\n",
    "    ds = PackedBinDataset(bin_prefix)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_packed)\n",
    "\n",
    "    b = next(iter(dl))\n",
    "    max_id = int(b[\"input_ids\"].max().item())\n",
    "    vocab_eff = max(vocab_nominal, max_id + 1)\n",
    "\n",
    "    cfg = {\n",
    "        \"vocab_size\": vocab_eff,\n",
    "        \"context_length\": ctx_len,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"dropout\": dropout,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"qkv_bias\": qkv_bias,\n",
    "        \"n_layers\": n_layers,\n",
    "    }\n",
    "    model = GPTModel(cfg).to(device)\n",
    "    if tie_weights:\n",
    "        model.output_layer.weight = model.embedding_layer.embedding.weight\n",
    "\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        if n.endswith(\"bias\") or (\"ln\" in n.lower()) or (\"layernorm\" in n.lower()):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    optim = AdamW([\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ], lr=lr)\n",
    "\n",
    "    total_steps = max_steps if (max_steps and max_steps > 0) else (epochs * len(dl))\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(1, warmup_steps)\n",
    "        return max(0.0, (total_steps - step) / max(1, total_steps - warmup_steps))\n",
    "    scheduler = LambdaLR(optim, lr_lambda)\n",
    "\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler('cuda' if device == 'cuda' else 'cpu')\n",
    "    except Exception:\n",
    "        scaler = GradScaler()\n",
    "\n",
    "    start_step = 0\n",
    "    if resume:\n",
    "        ckpt = torch.load(resume, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt.get(\"model\", {}), strict=False)\n",
    "        optim.load_state_dict(ckpt.get(\"optimizer\", {}))\n",
    "        try: scaler.load_state_dict(ckpt.get(\"scaler\", {}))\n",
    "        except: pass\n",
    "        start_step = int(ckpt.get(\"step\", 0))\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.train()\n",
    "    step = start_step\n",
    "    running, count = 0.0, 0\n",
    "\n",
    "    p_epoch = tqdm(total=total_steps, desc=\"训练\", unit=\"step\")\n",
    "    for _ in range(epochs):\n",
    "        for batch in dl:\n",
    "            step += 1\n",
    "\n",
    "            if device == \"cuda\":\n",
    "                _cuda_ctx = None\n",
    "                try:\n",
    "                    from torch.amp import autocast as _ac\n",
    "                    _cuda_ctx = _ac('cuda')\n",
    "                except Exception:\n",
    "                    _cuda_ctx = autocast()\n",
    "                with _cuda_ctx:\n",
    "                    logits = model(batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "                    loss = compute_lm_loss(logits, batch[\"labels\"].to(device)) / grad_accum\n",
    "            else:\n",
    "                logits = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "                loss = compute_lm_loss(logits, batch[\"labels\"]) / grad_accum\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if step % grad_accum == 0:\n",
    "                scaler.unscale_(optim)\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optim); scaler.update()\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "            running += loss.item()\n",
    "            count += 1\n",
    "            if log_every and step % log_every == 0:\n",
    "                avg = running * grad_accum / max(1, count)\n",
    "                ppl = math.exp(avg) if avg < 700 else float('inf')\n",
    "                lr_now = scheduler.get_last_lr()[0]\n",
    "                valid_tokens = int((batch[\"labels\"] != -100).sum().item())\n",
    "                p_epoch.set_postfix_str(f\"loss {avg:.4f} | ppl {ppl:.2f} | lr {lr_now:.6f} | valid {valid_tokens}\")\n",
    "                running, count = 0.0, 0\n",
    "\n",
    "            if save_every and step % save_every == 0:\n",
    "                path = os.path.join(save_dir, f\"ckpt_step_{step}.pt\")\n",
    "                _obj = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optim.state_dict(),\n",
    "                    \"scaler\": scaler.state_dict(),\n",
    "                    \"step\": step,\n",
    "                    \"cfg\": cfg,\n",
    "                }\n",
    "                try:\n",
    "                    torch.save(_obj, path)\n",
    "                except RuntimeError:\n",
    "                    torch.save(_obj, path, _use_new_zipfile_serialization=False)\n",
    "\n",
    "            p_epoch.update(1)\n",
    "            if step >= total_steps:\n",
    "                break\n",
    "    p_epoch.close()\n",
    "    return model\n",
    "\n",
    "def save_final_model(model, save_dir=r'G:\\Anaconda\\Kaggle\\gpt2\\checkpoints', file_name=\"final_model.pt\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    final_ckpt = os.path.join(save_dir, file_name)\n",
    "    sd_cpu = {}\n",
    "    for k, v in tqdm(model.state_dict().items(), desc=\"准备最终权重\", unit=\"param\"):\n",
    "        sd_cpu[k] = v.detach().cpu()\n",
    "    try:\n",
    "        torch.save({'model': sd_cpu}, final_ckpt)\n",
    "    except RuntimeError as e:\n",
    "        torch.save({'model': sd_cpu}, final_ckpt, _use_new_zipfile_serialization=False)\n",
    "    print(\"已保存:\", final_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c5aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19657\\AppData\\Local\\Temp\\ipykernel_41900\\1932365116.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb184cfd27d94787a4b0418bce69e7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练:   0%|          | 0/10000000000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19657\\AppData\\Local\\Temp\\ipykernel_41900\\1932365116.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnotebook_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbin_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpacked\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmix_2b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtokenizer_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbytebpe_zh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mctx_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_accum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mG:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAnaconda\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKaggle\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m save_final_model(model, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAnaconda\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKaggle\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 134\u001b[0m, in \u001b[0;36mnotebook_train\u001b[1;34m(bin_prefix, tokenizer_dir, ctx_len, emb_dim, n_heads, n_layers, dropout, qkv_bias, tie_weights, batch_size, num_workers, lr, weight_decay, warmup_steps, max_steps, grad_accum, log_every, save_every, save_dir, resume)\u001b[0m\n\u001b[0;32m    131\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 134\u001b[0m running \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_every \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = notebook_train(\n",
    "  bin_prefix=r\"G:\\Anaconda\\Kaggle\\gpt2\\data\\packed\\mix_2b\",\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  ctx_len=1024,\n",
    "  emb_dim=768, n_heads=12, n_layers=12, dropout=0.1, qkv_bias=False, tie_weights=True,\n",
    "  batch_size=1, num_workers=0,\n",
    "  lr=3e-4, weight_decay=0.1, warmup_steps=3000, max_steps=10000000000, grad_accum=1,\n",
    "  log_every=50, save_every=1000, save_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\", resume=\"\"\n",
    ")\n",
    "save_final_model(model, save_dir=r'G:\\Anaconda\\Kaggle\\gpt2\\checkpoints', file_name=\"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ceabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir=r'G:\\Anaconda\\Kaggle\\gpt2\\checkpoints'\n",
    "final_ckpt = os.path.join(save_dir, \"final_model.pt\")\n",
    "torch.save({\n",
    "        'model': model.state_dict(),\n",
    "    }, final_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039fcd6",
   "metadata": {},
   "source": [
    "## Step6 使用模型生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8e64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from model import GPTModel, load_tokenizer\n",
    "\n",
    "def load_model_from_ckpt(ckpt_path, device=None, dtype=None, cfg_override=None, tie_weights=True):\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    cfg = cfg_override\n",
    "    model = GPTModel(cfg)\n",
    "    if tie_weights:\n",
    "        try:\n",
    "            model.output_layer.weight = model.embedding_layer.embedding.weight\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        model.load_state_dict(ckpt['model'], strict=False)\n",
    "    except Exception:\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if dtype is not None:\n",
    "        model.to(dtype=dtype)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, cfg\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0):\n",
    "    if top_k and top_k > 0:\n",
    "        v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
    "        kth = v[:, -1].unsqueeze(-1)\n",
    "        logits = torch.where(logits < kth, torch.full_like(logits, -float('inf')), logits)\n",
    "    if top_p and top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "        sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, -float('inf'))\n",
    "    return logits\n",
    "\n",
    "def generate(prompt, ckpt_path, tokenizer_dir, max_new_tokens=200, temperature=1.0, top_k=50, top_p=0.95, stop_on_eos=True, device=None, use_half=True, cfg_override=None, tie_weights=True):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tok = load_tokenizer(tokenizer_dir)\n",
    "    eos_id = tok.eos_token_id\n",
    "    dtype = torch.float16 if (use_half and device == 'cuda') else None\n",
    "    model, cfg = load_model_from_ckpt(ckpt_path, device=device, dtype=dtype, cfg_override=cfg_override, tie_weights=tie_weights)\n",
    "    ids = tok(prompt, add_special_tokens=False, return_attention_mask=False)['input_ids']\n",
    "    if tok.bos_token_id is not None:\n",
    "        ids = [tok.bos_token_id] + ids\n",
    "    input_ids = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            x = input_ids[:, -cfg['context_length']:]\n",
    "            attn = torch.ones_like(x, dtype=torch.long)\n",
    "            logits = model(x, attention_mask=attn)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-5)\n",
    "            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            nid = int(next_id.item())\n",
    "            input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "            if stop_on_eos and eos_id is not None and nid == eos_id:\n",
    "                break\n",
    "    return tok.decode(input_ids[0].tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0b555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东南大学大学是江苏省理工学院的招生专业之一，由学校教务处负责管理。目前，东南大学在实行平行志愿录取工作，并组织开展“补录”。\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "  \"vocab_size\": 50002,\n",
    "  \"context_length\": 1024,\n",
    "  \"emb_dim\": 768,\n",
    "  \"dropout\": 0.1,\n",
    "  \"n_heads\": 12,\n",
    "  \"qkv_bias\": False,\n",
    "  \"n_layers\": 12,\n",
    "}\n",
    "text = generate(\n",
    "  prompt=\"东南大学\",\n",
    "  ckpt_path=r\"G:\\Anaconda\\Kaggle\\gpt2\\checkpoints\\final_model.pt\",\n",
    "  tokenizer_dir=r\"G:\\Anaconda\\Kaggle\\gpt2\\tokenizer\\bytebpe_zh\",\n",
    "  max_new_tokens=512,\n",
    "  temperature=0.8,\n",
    "  top_k=30, top_p=0.9,\n",
    "  stop_on_eos=True,\n",
    "  device='cuda',\n",
    "  use_half=True,\n",
    "  cfg_override=cfg,\n",
    "  tie_weights=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0e442",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
