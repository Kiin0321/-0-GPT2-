{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa13892",
   "metadata": {},
   "source": [
    "# 124M的简单GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77c131",
   "metadata": {},
   "source": [
    "## Step1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da12e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b933caa",
   "metadata": {},
   "source": [
    "## Step2 初始参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"dropout\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819cc73",
   "metadata": {},
   "source": [
    "## Step3 分块构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c0126",
   "metadata": {},
   "source": [
    "### 1.构建embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d7bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,max_length,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length,embedding_dim)\n",
    "    def forward(self,input):\n",
    "        input_embeddings = self.embedding(input)\n",
    "        seq_len = input_embeddings.size(-2)\n",
    "        positions = torch.arange(seq_len, device=input_embeddings.device)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        output = input_embeddings + position_embeddings\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f55da",
   "metadata": {},
   "source": [
    "### 2.构建注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "171c5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,context_length,dropout , qkv_bias):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_q = nn.Linear(embedding_dim , embedding_dim , bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(embedding_dim , embedding_dim , bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(embedding_dim , embedding_dim , bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #因果权重\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length, dtype=torch.bool), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        #提取batch的大小、token的数量、跟宽度\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        #进行运算计算\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        #通过点积来计算attention的数值\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1## 缩放因子 √d，用于稳定梯度\n",
    "        )\n",
    "        #在时间顺序上进行mask确保信息不会被泄露\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "        #防止过拟合的dropout处理方式\n",
    "        context_vec = attn_weights @ values\n",
    "        # 根据注意力权重计算上下文向量\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aecb2a",
   "metadata": {},
   "source": [
    "### 3.构建多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd45d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        #确保是可以被整除的\n",
    "            \n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "        #初始化头的维度、数量\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        #头的输出结合线性层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #进行dropout防止过拟合\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length, dtype=torch.bool),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        # 上三角掩码，确保因果性\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        #把输出的维度拆成头*头大小\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        #转制维度,听说是为了更好的计算注意力\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # 计算缩放点积注意力\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        # 将掩码缩减到当前 token 数量，并转换为布尔型\n",
    "        # 进而实现动态遮蔽,所以不用另开好几个数组\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # 遮蔽矩阵\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        #归一化\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        #头的合并\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        #对上下文向量的形状进行调整，确保输出的形状\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63083e",
   "metadata": {},
   "source": [
    "### 4.构建基于多头注意力的Transformer层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77124d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTransformerLayer(nn.Module):\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention_layer = MultiHeadAttention(\n",
    "            d_in,\n",
    "            d_out,\n",
    "            context_length,\n",
    "            dropout,\n",
    "            num_heads,\n",
    "            qkv_bias\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_out , 4*d_out),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*d_out , d_out)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_out)\n",
    "        self.ln2 = nn.LayerNorm(d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self , x):\n",
    "        # 两次残差\n",
    "        y_1 = self.dropout(self.attention_layer(self.ln1(x)))\n",
    "        y_1 += x\n",
    "\n",
    "        x = y_1\n",
    "        y_2 = self.dropout(self.fc(self.ln2(x)))\n",
    "        y_2 += x\n",
    "        return y_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eafda7",
   "metadata": {},
   "source": [
    "## Step4 完成GPT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85461db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(cfg[\"vocab_size\"] , cfg[\"context_length\"] , cfg[\"emb_dim\"])\n",
    "        self.transformers_blks = nn.Sequential(\n",
    "            *[GPTTransformerLayer(\n",
    "                d_in = cfg[\"emb_dim\"],\n",
    "                d_out = cfg[\"emb_dim\"],\n",
    "                context_length = cfg[\"context_length\"],\n",
    "                dropout = cfg[\"dropout\"],\n",
    "                num_heads = cfg[\"n_heads\"],\n",
    "                qkv_bias = cfg[\"qkv_bias\"]\n",
    "            ) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.embed_drop = nn.Dropout(cfg[\"dropout\"])\n",
    "        self.ln = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output_layer = nn.Linear(cfg[\"emb_dim\"] , cfg[\"vocab_size\"],bias = False)\n",
    "    def forward(self,x):\n",
    "        embedding_output = self.embedding_layer(x)\n",
    "        embedding_output = self.embed_drop(embedding_output)\n",
    "        transformers_output = self.transformers_blks(embedding_output)\n",
    "        output = self.ln(transformers_output)\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96966f3b",
   "metadata": {},
   "source": [
    "## Step5 测试模型参数量及模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f8a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "#模型的总参数数量\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.output_layer.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "#Parameter- sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75692d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # 预测单词的模块\n",
    "    # idx 是当前上下文中的（batch, n_tokens）索引数组\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 每次生成一个单词后，重新将其加入序列中\n",
    "        # 如果当前上下文长度超过模型支持的最大上下文长度，则截取\n",
    "        # 例如，如果LLM只支持5个token，而上下文长度为10\n",
    "        # 那么只使用最后5个token作为上下文\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # 如果idx的长度超过模型支持的上下文长度size，只保留最后size个token\n",
    "        # 避免溢出\n",
    "        # 获取预测结果\n",
    "        with torch.no_grad():  # 在推理阶段，不需要计算梯度，因为没有反向传播\n",
    "            # 这样可以减少存储开销\n",
    "            logits = model(idx_cond)\n",
    "            # 模型输出结果\n",
    "        # 只关注最后一个时间步的输出\n",
    "        # (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        # 关注最后一个时间步\n",
    "        # 使用softmax函数计算概率\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        # 归一化\n",
    "        # 获取具有最高概率值的词汇索引\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "        # 获取概率最高的词汇索引\n",
    "        # 将采样的索引添加到序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef31fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [22755, 239, 42468, 171, 120, 234, 33768, 254, 46763, 234, 41098, 88]\n",
      "encoded_tensor.shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")#初始化GPT2!\n",
    "start_context = \"我是，无敌jjy\"\n",
    "#模拟\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "#进行语义理解\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfe44ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[22755,   239, 42468,   171,   120,   234, 33768,   254, 46763,   234,\n",
      "         41098,    88,  8549, 37595, 43749, 47275, 39866, 16217]])\n",
      "Output length: 18\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "#在检验的时候不需要正则化了\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    #左边的参数名字,右边是函数传入的实际模型\n",
    "    idx=encoded_tensor, #上下文的索引\n",
    "    max_new_tokens=6, #最多运行六次,然后取结果概率最高的\n",
    "    #初始文本➕6\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c192c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是，无敌jjy Managementarf CLI revocationPolit immun\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32261232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms4scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
