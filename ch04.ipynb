{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477cfc18",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754e00c",
   "metadata": {},
   "source": [
    "## Step1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5299644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Anaconda\\envs\\llms4scratch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import matplotlib \n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cc3ed",
   "metadata": {},
   "source": [
    "## Step2 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7cdd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 1024, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"dropout\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73df2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e826c6",
   "metadata": {},
   "source": [
    "## Step3 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "#引入数据集\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "#一系列经典的读取数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a120c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "print(len(text_data))\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac4c64",
   "metadata": {},
   "source": [
    "## Step4 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b366beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "encoded_token = tokenizer.encode(text_data)\n",
    "print(len(encoded_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de69477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #生成encoded\n",
    "        encoded = tokenizer.encode(txt)\n",
    "\n",
    "        #生成数据\n",
    "        for i in range(0,len(encoded)-max_length,stride):\n",
    "            input = encoded[i:i+max_length]\n",
    "            target = encoded[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input))\n",
    "            self.target_ids.append(torch.tensor(target))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index],self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515c3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091c0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_sample = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:train_sample]\n",
    "test_data = text_data[train_sample:]\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = 128,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size = 4,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = 128,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665f9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(encoded_token)* (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if len(encoded_token) * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abab329",
   "metadata": {},
   "source": [
    "## Step5 构建训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300d96b",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf889c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    #呼唤GPU\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    #用交叉熵函数对于logits进行计算并且拉伸到二维长度\n",
    "    return loss\n",
    "#一个计算批损失的函数\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果指定的批次数超过数据加载器中的总批次数，则将批次数减少到与数据加载器的总批次数匹配。\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        #减少需要处理的数量,同时也是防止溢出\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "        #一点点加上去的损失\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea602e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.003416220347086\n",
      "Validation loss: 11.027277946472168\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 如果支持，则调用 GPU\n",
    "\n",
    "# 注意：\n",
    "# 如果取消注释以下代码块，代码可以在 Apple Silicon 芯片上运行（如果适用），\n",
    "# 在 M3 MacBook Air 上测量速度大约是 Apple CPU 的两倍。\n",
    "# 然而，计算得到的损失值可能会略有不同。\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)  # 对于 nn.Module 类，不需要赋值 model = model.to(device)\n",
    "\n",
    "torch.manual_seed(123)  # 固定随机种子，保证数据加载器打乱数据的结果可复现\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度跟踪以提高效率，因为此时尚未开始训练\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(test_loader, model, device)\n",
    "\n",
    "# 推理阶段不计算梯度\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    #初始化训练模型而且给了空的队列\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):#训练次数\n",
    "        model.train()  # Set model to training mode\n",
    "        #转移到训练模块\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            #从loader里面调出输入跟目标\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            #清空所有函数的梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            #计算损失函数\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            #反向传播优化\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            #更新权重\n",
    "            tokens_seen += input_batch.numel()\n",
    "            #加一下一共有多少\n",
    "            global_step += 1\n",
    "            #看一下一共训练了多少步\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                #按照一定的步数进行记录\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                #计算损失函数\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                #加到list中\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    #评价模块\n",
    "    model.eval()\n",
    "    #检验模式\n",
    "    with torch.no_grad():\n",
    "        #我认为的双保险,防止梯度更新\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    #\t在评估结束后切换回训练模式，确保模型能继续用于训练。\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "#下面可以看一下计算了多久\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "#经典操作\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "#用Adam进行优化,其中学习rate为0.004,动量衰减是0.1\n",
    "num_epochs = 1\n",
    "#10论学习\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, test_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "#一个经典的plot画图函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c312a8",
   "metadata": {},
   "source": [
    "##  Step6 测试模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"我是无敌jjy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9efae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "start_context = \"你好，我是无敌jjy\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "print(\"Output text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a934fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7c0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms4scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
