{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8ba1c6",
   "metadata": {},
   "source": [
    "# 处理文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794650a",
   "metadata": {},
   "source": [
    "## Step1 导包并测试版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a84c483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "from modelscope.msdatasets import MsDataset\n",
    "import jieba\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71125293",
   "metadata": {},
   "source": [
    "## Step2 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b729d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "#数据集下载\n",
    "dataset = MsDataset.load('swift/Chinese-Qwen3-235B-2507-Distill-data-110k-SFT',cache_dir='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532b2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '能给我讲一个寓意深刻的故事吗？'}, {'role': 'assistant', 'content': '当然可以。下面是一个寓意深刻的小故事，名为《石匠与神像》：\\n\\n很久以前，在一座深山里，有一位技艺精湛的石匠，名叫阿岩。他一生都在雕刻石头，作品精美绝伦，远近闻名。但他从不雕刻神像，别人问他为什么，他总是沉默不语。\\n\\n一天，村里要修建一座庙宇，村长请他雕刻一尊守护神像。阿岩犹豫了很久，终于答应了。他选了一块巨大的青石，开始日复一日地凿刻。每一锤、每一凿，他都极为专注，仿佛在与石头对话。\\n\\n三年过去了，神像终于完成。它面容庄严，目光深邃，仿佛能看透人心。村民们都惊叹不已，纷纷跪拜，称其为“活的神”。\\n\\n落成典礼那天，村长对阿岩说：“你创造了神，你就是神的化身。”\\n\\n阿岩却摇了摇头，轻声说：“不，我不是创造者，我只是唤醒了它。”\\n\\n众人不解。阿岩走到神像前，指着一处细微的裂痕说：“你们看，这块石头原本有一道裂纹，藏在深处。我本想避开它，可无论怎么雕，那裂痕总会出现在神像的眉心。最后我明白——它不是瑕疵，而是它本来的样子。”\\n\\n他顿了顿，望向群山：“这石头在山中沉睡了千年，经历了风雨雷电，才有了这道伤痕。我所做的，不是塑造完美，而是让伤痕也成为神圣的一部分。”\\n\\n众人沉默。\\n\\n阿岩又说：“人总想雕刻出完美的神，可真正的神，不是无瑕的偶像，而是包容伤痕、接纳残缺的存在。我们每个人心中都有一块石头，有裂痕，有沉重，有沉默。但正是这些，让我们真实，让我们值得被唤醒。”\\n\\n说完，他放下刻刀，走进山林，再也没有回来。\\n\\n——\\n\\n**寓意**：  \\n这个故事告诉我们，真正的美与力量，不在于完美无缺，而在于接纳自己的伤痕与不完美。每个人都有裂痕，但那不是缺陷，而是经历的印记，是灵魂的深度。我们不必成为别人眼中的“完美神像”，而是要勇敢地“唤醒”真实的自己——带着伤痕，依然庄严站立。'}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd191f",
   "metadata": {},
   "source": [
    "## Step3 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000 110000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def split_user_assistant(ds):\n",
    "    user_rows = []\n",
    "    assistant_rows = []\n",
    "    train = ds['train'] if isinstance(ds, dict) and 'train' in ds else ds\n",
    "    for item in train:\n",
    "        if isinstance(item, dict):\n",
    "            convo_key = None\n",
    "            for k in ['conversations','messages','dialog','chat','conversation']:\n",
    "                if k in item and isinstance(item[k], (list,tuple)):\n",
    "                    convo_key = k\n",
    "                    break\n",
    "            if convo_key:\n",
    "                for m in item[convo_key]:\n",
    "                    role = m.get('role') if isinstance(m, dict) else None\n",
    "                    content = m.get('content') if isinstance(m, dict) else (m[1] if isinstance(m,(list,tuple)) and len(m)>1 else None)\n",
    "                    if role == 'user' and content is not None:\n",
    "                        user_rows.append({'content': content})\n",
    "                    elif role == 'assistant' and content is not None:\n",
    "                        assistant_rows.append({'content': content})\n",
    "                continue\n",
    "            found = False\n",
    "            for ukey, akey in [('instruction','output'),('prompt','response'),('question','answer'),('input','output'),('source','target'),('query','response'),('user','assistant')]:\n",
    "                if ukey in item and akey in item:\n",
    "                    user_rows.append({'content': item[ukey]})\n",
    "                    assistant_rows.append({'content': item[akey]})\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                pass\n",
    "    user_df = pd.DataFrame(user_rows)\n",
    "    assistant_df = pd.DataFrame(assistant_rows)\n",
    "    return user_df, assistant_df\n",
    "user_df, assistant_df = split_user_assistant(dataset)\n",
    "user_list = user_df['content'].tolist()\n",
    "assistant_list = assistant_df['content'].tolist()\n",
    "print(len(user_list), len(assistant_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630b1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\19657\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.613 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['能', '给', '我', '讲', '一个', '寓意', '深刻', '的', '故事', '吗', '？']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = jieba\n",
    "user_res = list(tokenizer.cut(user_list[0]))\n",
    "user_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tqdm_tokenize",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110000/110000 [06:38<00:00, 276.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['能', '给', '我', '讲', '一个', '寓意', '深刻', '的', '故事', '吗', '？'],\n",
       " ['当然',\n",
       "  '可以',\n",
       "  '。',\n",
       "  '下面',\n",
       "  '是',\n",
       "  '一个',\n",
       "  '寓意',\n",
       "  '深刻',\n",
       "  '的',\n",
       "  '小',\n",
       "  '故事',\n",
       "  '，',\n",
       "  '名为',\n",
       "  '《',\n",
       "  '石匠',\n",
       "  '与',\n",
       "  '神像',\n",
       "  '》',\n",
       "  '：',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '很久以前',\n",
       "  '，',\n",
       "  '在',\n",
       "  '一座',\n",
       "  '深山',\n",
       "  '里',\n",
       "  '，',\n",
       "  '有',\n",
       "  '一位',\n",
       "  '技艺',\n",
       "  '精湛',\n",
       "  '的',\n",
       "  '石匠',\n",
       "  '，',\n",
       "  '名叫',\n",
       "  '阿岩',\n",
       "  '。',\n",
       "  '他',\n",
       "  '一生',\n",
       "  '都',\n",
       "  '在',\n",
       "  '雕刻',\n",
       "  '石头',\n",
       "  '，',\n",
       "  '作品',\n",
       "  '精美绝伦',\n",
       "  '，',\n",
       "  '远近闻名',\n",
       "  '。',\n",
       "  '但',\n",
       "  '他',\n",
       "  '从不',\n",
       "  '雕刻',\n",
       "  '神像',\n",
       "  '，',\n",
       "  '别人',\n",
       "  '问',\n",
       "  '他',\n",
       "  '为什么',\n",
       "  '，',\n",
       "  '他',\n",
       "  '总是',\n",
       "  '沉默不语',\n",
       "  '。',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '一天',\n",
       "  '，',\n",
       "  '村里',\n",
       "  '要',\n",
       "  '修建',\n",
       "  '一座',\n",
       "  '庙宇',\n",
       "  '，',\n",
       "  '村长',\n",
       "  '请',\n",
       "  '他',\n",
       "  '雕刻',\n",
       "  '一尊',\n",
       "  '守护神',\n",
       "  '像',\n",
       "  '。',\n",
       "  '阿岩',\n",
       "  '犹豫',\n",
       "  '了',\n",
       "  '很',\n",
       "  '久',\n",
       "  '，',\n",
       "  '终于',\n",
       "  '答应',\n",
       "  '了',\n",
       "  '。',\n",
       "  '他选',\n",
       "  '了',\n",
       "  '一块',\n",
       "  '巨大',\n",
       "  '的',\n",
       "  '青石',\n",
       "  '，',\n",
       "  '开始',\n",
       "  '日复一日',\n",
       "  '地凿刻',\n",
       "  '。',\n",
       "  '每',\n",
       "  '一锤',\n",
       "  '、',\n",
       "  '每一凿',\n",
       "  '，',\n",
       "  '他',\n",
       "  '都',\n",
       "  '极为',\n",
       "  '专注',\n",
       "  '，',\n",
       "  '仿佛',\n",
       "  '在',\n",
       "  '与',\n",
       "  '石头',\n",
       "  '对话',\n",
       "  '。',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '三年',\n",
       "  '过去',\n",
       "  '了',\n",
       "  '，',\n",
       "  '神像',\n",
       "  '终于',\n",
       "  '完成',\n",
       "  '。',\n",
       "  '它',\n",
       "  '面容',\n",
       "  '庄严',\n",
       "  '，',\n",
       "  '目光',\n",
       "  '深邃',\n",
       "  '，',\n",
       "  '仿佛',\n",
       "  '能',\n",
       "  '看透',\n",
       "  '人心',\n",
       "  '。',\n",
       "  '村民',\n",
       "  '们',\n",
       "  '都',\n",
       "  '惊叹不已',\n",
       "  '，',\n",
       "  '纷纷',\n",
       "  '跪拜',\n",
       "  '，',\n",
       "  '称其为',\n",
       "  '“',\n",
       "  '活',\n",
       "  '的',\n",
       "  '神',\n",
       "  '”',\n",
       "  '。',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '落成典礼',\n",
       "  '那天',\n",
       "  '，',\n",
       "  '村长',\n",
       "  '对',\n",
       "  '阿岩',\n",
       "  '说',\n",
       "  '：',\n",
       "  '“',\n",
       "  '你',\n",
       "  '创造',\n",
       "  '了',\n",
       "  '神',\n",
       "  '，',\n",
       "  '你',\n",
       "  '就是',\n",
       "  '神',\n",
       "  '的',\n",
       "  '化身',\n",
       "  '。',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '阿岩',\n",
       "  '却',\n",
       "  '摇',\n",
       "  '了',\n",
       "  '摇头',\n",
       "  '，',\n",
       "  '轻声',\n",
       "  '说',\n",
       "  '：',\n",
       "  '“',\n",
       "  '不',\n",
       "  '，',\n",
       "  '我',\n",
       "  '不是',\n",
       "  '创造者',\n",
       "  '，',\n",
       "  '我',\n",
       "  '只是',\n",
       "  '唤醒',\n",
       "  '了',\n",
       "  '它',\n",
       "  '。',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '众人',\n",
       "  '不解',\n",
       "  '。',\n",
       "  '阿岩',\n",
       "  '走',\n",
       "  '到',\n",
       "  '神像',\n",
       "  '前',\n",
       "  '，',\n",
       "  '指着',\n",
       "  '一处',\n",
       "  '细微',\n",
       "  '的',\n",
       "  '裂痕',\n",
       "  '说',\n",
       "  '：',\n",
       "  '“',\n",
       "  '你们',\n",
       "  '看',\n",
       "  '，',\n",
       "  '这块',\n",
       "  '石头',\n",
       "  '原本',\n",
       "  '有',\n",
       "  '一道',\n",
       "  '裂纹',\n",
       "  '，',\n",
       "  '藏',\n",
       "  '在',\n",
       "  '深处',\n",
       "  '。',\n",
       "  '我本',\n",
       "  '想',\n",
       "  '避开',\n",
       "  '它',\n",
       "  '，',\n",
       "  '可',\n",
       "  '无论',\n",
       "  '怎么',\n",
       "  '雕',\n",
       "  '，',\n",
       "  '那',\n",
       "  '裂痕',\n",
       "  '总会',\n",
       "  '出现',\n",
       "  '在',\n",
       "  '神像',\n",
       "  '的',\n",
       "  '眉心',\n",
       "  '。',\n",
       "  '最后',\n",
       "  '我',\n",
       "  '明白',\n",
       "  '—',\n",
       "  '—',\n",
       "  '它',\n",
       "  '不是',\n",
       "  '瑕疵',\n",
       "  '，',\n",
       "  '而是',\n",
       "  '它',\n",
       "  '本来',\n",
       "  '的',\n",
       "  '样子',\n",
       "  '。',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '他顿',\n",
       "  '了',\n",
       "  '顿',\n",
       "  '，',\n",
       "  '望',\n",
       "  '向',\n",
       "  '群山',\n",
       "  '：',\n",
       "  '“',\n",
       "  '这',\n",
       "  '石头',\n",
       "  '在',\n",
       "  '山中',\n",
       "  '沉睡',\n",
       "  '了',\n",
       "  '千年',\n",
       "  '，',\n",
       "  '经历',\n",
       "  '了',\n",
       "  '风雨雷电',\n",
       "  '，',\n",
       "  '才',\n",
       "  '有',\n",
       "  '了',\n",
       "  '这道',\n",
       "  '伤痕',\n",
       "  '。',\n",
       "  '我',\n",
       "  '所',\n",
       "  '做',\n",
       "  '的',\n",
       "  '，',\n",
       "  '不是',\n",
       "  '塑造',\n",
       "  '完美',\n",
       "  '，',\n",
       "  '而是',\n",
       "  '让',\n",
       "  '伤痕',\n",
       "  '也',\n",
       "  '成为',\n",
       "  '神圣',\n",
       "  '的',\n",
       "  '一部分',\n",
       "  '。',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '众人',\n",
       "  '沉默',\n",
       "  '。',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '阿岩',\n",
       "  '又',\n",
       "  '说',\n",
       "  '：',\n",
       "  '“',\n",
       "  '人',\n",
       "  '总想',\n",
       "  '雕刻',\n",
       "  '出',\n",
       "  '完美',\n",
       "  '的',\n",
       "  '神',\n",
       "  '，',\n",
       "  '可',\n",
       "  '真正',\n",
       "  '的',\n",
       "  '神',\n",
       "  '，',\n",
       "  '不是',\n",
       "  '无瑕',\n",
       "  '的',\n",
       "  '偶像',\n",
       "  '，',\n",
       "  '而是',\n",
       "  '包容',\n",
       "  '伤痕',\n",
       "  '、',\n",
       "  '接纳',\n",
       "  '残缺',\n",
       "  '的',\n",
       "  '存在',\n",
       "  '。',\n",
       "  '我们',\n",
       "  '每个',\n",
       "  '人',\n",
       "  '心中',\n",
       "  '都',\n",
       "  '有',\n",
       "  '一块',\n",
       "  '石头',\n",
       "  '，',\n",
       "  '有',\n",
       "  '裂痕',\n",
       "  '，',\n",
       "  '有',\n",
       "  '沉重',\n",
       "  '，',\n",
       "  '有',\n",
       "  '沉默',\n",
       "  '。',\n",
       "  '但',\n",
       "  '正是',\n",
       "  '这些',\n",
       "  '，',\n",
       "  '让',\n",
       "  '我们',\n",
       "  '真实',\n",
       "  '，',\n",
       "  '让',\n",
       "  '我们',\n",
       "  '值得',\n",
       "  '被',\n",
       "  '唤醒',\n",
       "  '。',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '说完',\n",
       "  '，',\n",
       "  '他',\n",
       "  '放下',\n",
       "  '刻刀',\n",
       "  '，',\n",
       "  '走进',\n",
       "  '山林',\n",
       "  '，',\n",
       "  '再也',\n",
       "  '没有',\n",
       "  '回来',\n",
       "  '。',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '—',\n",
       "  '—',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '*',\n",
       "  '*',\n",
       "  '寓意',\n",
       "  '*',\n",
       "  '*',\n",
       "  '：',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '\\n',\n",
       "  '这个',\n",
       "  '故事',\n",
       "  '告诉',\n",
       "  '我们',\n",
       "  '，',\n",
       "  '真正',\n",
       "  '的',\n",
       "  '美',\n",
       "  '与',\n",
       "  '力量',\n",
       "  '，',\n",
       "  '不',\n",
       "  '在于',\n",
       "  '完美无缺',\n",
       "  '，',\n",
       "  '而',\n",
       "  '在于',\n",
       "  '接纳',\n",
       "  '自己',\n",
       "  '的',\n",
       "  '伤痕',\n",
       "  '与',\n",
       "  '不',\n",
       "  '完美',\n",
       "  '。',\n",
       "  '每个',\n",
       "  '人',\n",
       "  '都',\n",
       "  '有',\n",
       "  '裂痕',\n",
       "  '，',\n",
       "  '但',\n",
       "  '那',\n",
       "  '不是',\n",
       "  '缺陷',\n",
       "  '，',\n",
       "  '而是',\n",
       "  '经历',\n",
       "  '的',\n",
       "  '印记',\n",
       "  '，',\n",
       "  '是',\n",
       "  '灵魂',\n",
       "  '的',\n",
       "  '深度',\n",
       "  '。',\n",
       "  '我们',\n",
       "  '不必',\n",
       "  '成为',\n",
       "  '别人',\n",
       "  '眼中',\n",
       "  '的',\n",
       "  '“',\n",
       "  '完美',\n",
       "  '神像',\n",
       "  '”',\n",
       "  '，',\n",
       "  '而是',\n",
       "  '要',\n",
       "  '勇敢',\n",
       "  '地',\n",
       "  '“',\n",
       "  '唤醒',\n",
       "  '”',\n",
       "  '真实',\n",
       "  '的',\n",
       "  '自己',\n",
       "  '—',\n",
       "  '—',\n",
       "  '带',\n",
       "  '着',\n",
       "  '伤痕',\n",
       "  '，',\n",
       "  '依然',\n",
       "  '庄严',\n",
       "  '站立',\n",
       "  '。'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "user_res, assistant_res = [], []\n",
    "for user, assistant in tqdm(zip(user_list, assistant_list), total=len(user_list)):\n",
    "    user_res.append(list(tokenizer.cut(user)))\n",
    "    assistant_res.append(list(tokenizer.cut(assistant)))\n",
    "user_res[0], assistant_res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a194ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r'G:\\Anaconda\\Kaggle\\LLMs-from-scratch-main\\my_test_ch01\\data\\tokenized_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'user_tokenized': user_res,\n",
    "        'assistant_tokenized': assistant_res\n",
    "    }, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6844c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging words: 100%|██████████| 110000/110000 [00:04<00:00, 27357.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'能'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#合并两个词表\n",
    "all_words = []\n",
    "for i in tqdm(range(len(user_res)), desc=\"Merging words\"):\n",
    "    all_words += user_res[i] + assistant_res[i]\n",
    "\n",
    "all_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c559590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490092"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去重，计算词表大小\n",
    "all_words = sorted(set(all_words))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e90a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\x01', 0)\n",
      "('\\x08', 1)\n",
      "('\\t', 2)\n",
      "('\\n', 3)\n",
      "('\\x0c', 4)\n",
      "('\\r', 5)\n",
      "(' ', 6)\n",
      "('!', 7)\n",
      "('\"', 8)\n",
      "('#', 9)\n",
      "('##', 10)\n",
      "('###', 11)\n",
      "('####', 12)\n",
      "('#####', 13)\n",
      "('######', 14)\n",
      "('##_', 15)\n",
      "('##_##', 16)\n",
      "('##__', 17)\n",
      "('#%', 18)\n",
      "('#&', 19)\n",
      "('#+', 20)\n",
      "('#++', 21)\n",
      "('#-', 22)\n",
      "('#.', 23)\n",
      "('#.##', 24)\n",
      "('#__', 25)\n",
      "('$', 26)\n",
      "('%', 27)\n",
      "('%%', 28)\n",
      "('%%%', 29)\n",
      "('%&', 30)\n",
      "('%+', 31)\n",
      "('%-', 32)\n",
      "('%.', 33)\n",
      "('%.%', 34)\n",
      "('%._', 35)\n",
      "('%_', 36)\n",
      "('&', 37)\n",
      "('&#', 38)\n",
      "('&#...', 39)\n",
      "('&&', 40)\n",
      "('&&...', 41)\n",
      "('&+', 42)\n",
      "('&-', 43)\n",
      "('&--', 44)\n",
      "('&--#', 45)\n",
      "('&.', 46)\n",
      "('&...', 47)\n",
      "('&_', 48)\n",
      "('&__', 49)\n"
     ]
    }
   ],
   "source": [
    "#构建索引映射\n",
    "vocab = {token :index for index , token in enumerate(all_words)}\n",
    "for i , item in enumerate(vocab.items()):\n",
    "    if i >=50:\n",
    "        break\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc159d",
   "metadata": {},
   "source": [
    "## Step3 构建简单分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15bf20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer():\n",
    "    def __init__(self,vocab,tokenizer):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {val:key for key , val in vocab.items()}\n",
    "        self.tokenizer = tokenizer\n",
    "    def encoder(self,sequence):\n",
    "        cut_seq = list(self.tokenizer.cut(sequence))\n",
    "        result = []\n",
    "        for i in range(len(cut_seq)):\n",
    "            result.append(self.str_to_int[cut_seq[i]])\n",
    "        return result\n",
    "    def decoder(self,tokenized_result):\n",
    "        result = \"\"\n",
    "        for i in range(len(tokenized_result)):\n",
    "            result += self.int_to_str[tokenized_result[i]]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb57c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'能给我讲一个寓意深刻的故事吗？'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f384d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[421098,\n",
       " 412376,\n",
       " 312643,\n",
       " 440095,\n",
       " 178785,\n",
       " 285388,\n",
       " 369614,\n",
       " 391059,\n",
       " 328683,\n",
       " 251846,\n",
       " 489130]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_tokenizer = SimpleTokenizer(vocab,tokenizer)\n",
    "encoded = simple_tokenizer.encoder(user_list[0])\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e05dc709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'能给我讲一个寓意深刻的故事吗？'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = simple_tokenizer.decoder(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113f88a",
   "metadata": {},
   "source": [
    "## Step5 复杂的情况及分词器的改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0286d4e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'jjy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moh！jjy，童话里做英雄！oh，jjy，热血心中流动\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43msimple_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m encoded\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#很多情况，构建词表还是会存在切词后词不在词表中的情况\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#并且在实际nlp任务处理文本的时候，还会在开始，结尾处增加特殊标记\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizer.encoder\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cut_seq)):\n\u001b[1;32m---> 10\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcut_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyError\u001b[0m: 'jjy'"
     ]
    }
   ],
   "source": [
    "text = \"oh！jjy，童话里做英雄！oh，jjy，热血心中流动\"\n",
    "encoded = simple_tokenizer.encoder(text)\n",
    "encoded\n",
    "#很多情况，构建词表还是会存在切词后词不在词表中的情况\n",
    "#并且在实际nlp任务处理文本的时候，还会在开始，结尾处增加特殊标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a9dc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'能给我讲一个寓意深刻的故事吗？'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = simple_tokenizer.decoder(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f601ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加特殊标记的vocab\n",
    "vocab_len = len(vocab.keys()) # 490092\n",
    "vocab[\"<bos>\"] = 490092\n",
    "vocab[\"<eos>\"] = 490093\n",
    "vocab[\"<unk>\"] = 490094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a9c9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#新的切词器\n",
    "class NewTokenizer():\n",
    "    def __init__(self,vocab,tokenizer):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {val:key for key , val in vocab.items()}\n",
    "        self.tokenizer = tokenizer\n",
    "    def encoder(self,sequence):\n",
    "        cut_seq = list(self.tokenizer.cut(sequence))\n",
    "        result = []\n",
    "        result.append(self.str_to_int[\"<bos>\"])\n",
    "        for i in range(len(cut_seq)):\n",
    "            if cut_seq[i] in self.str_to_int.keys():\n",
    "                result.append(self.str_to_int[cut_seq[i]])\n",
    "            else:\n",
    "                result.append(self.str_to_int[\"<unk>\"])\n",
    "        result.append(self.str_to_int[\"eos\"])\n",
    "        return result\n",
    "    def decoder(self,encoded):\n",
    "        result = \"\"\n",
    "        for i in range(len(encoded)):\n",
    "            result += self.int_to_str[encoded[i]]\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43feef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = NewTokenizer(vocab,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39f50f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[490092,\n",
       " 154009,\n",
       " 489102,\n",
       " 490094,\n",
       " 489111,\n",
       " 403248,\n",
       " 462325,\n",
       " 214026,\n",
       " 428341,\n",
       " 489102,\n",
       " 154009,\n",
       " 489111,\n",
       " 490094,\n",
       " 489111,\n",
       " 376565,\n",
       " 305885,\n",
       " 366869,\n",
       " 134573]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"oh！jjy，童话里做英雄！oh，jjy，热血心中流动\"\n",
    "encoded = new_tokenizer.encoder(text)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c445cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1de5afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>oh！<unk>，童话里做英雄！oh，<unk>，热血心中流动eos'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = new_tokenizer.decoder(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206bc50",
   "metadata": {},
   "source": [
    "## Step5 BPE分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a76e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用jieba分词会导致大规模的词模型没见过，从而无法准确完成encoder和decoder的任务\n",
    "因此使用BPE分词\n",
    "\"\"\"\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "677dcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")#初始化GPT2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bb76c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1219,\n",
       " 11,\n",
       " 41098,\n",
       " 88,\n",
       " 0,\n",
       " 44165,\n",
       " 98,\n",
       " 46237,\n",
       " 251,\n",
       " 34932,\n",
       " 234,\n",
       " 161,\n",
       " 223,\n",
       " 248,\n",
       " 164,\n",
       " 233,\n",
       " 109,\n",
       " 37239,\n",
       " 226,\n",
       " 171,\n",
       " 120,\n",
       " 223,\n",
       " 1219,\n",
       " 171,\n",
       " 120,\n",
       " 234,\n",
       " 41098,\n",
       " 88,\n",
       " 171,\n",
       " 120,\n",
       " 234,\n",
       " 163,\n",
       " 225,\n",
       " 255,\n",
       " 26193,\n",
       " 222,\n",
       " 33232,\n",
       " 225,\n",
       " 40792,\n",
       " 38184,\n",
       " 223,\n",
       " 27950,\n",
       " 101,\n",
       " 171,\n",
       " 120,\n",
       " 223]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"oh,jjy!童话里做英雄！oh，jjy，热血心中流动！\"\n",
    "encoded = tokenizer.encode(text)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6782ec84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh,jjy!童话里做英雄！oh，jjy，热血心中流动！'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5131ff93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'能给我讲一个寓意深刻的故事吗？'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82645d",
   "metadata": {},
   "source": [
    "## Step6 滑动窗口预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b44cf788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(user_list[0])\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8936ef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([47797, 121, 163, 119], [121, 163, 119, 247])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 4#上下文长度\n",
    "first = encoded[:context]\n",
    "second = encoded[1:context+1]\n",
    "first , second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a369381",
   "metadata": {},
   "source": [
    "## Step7 构建GPT数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3999ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #生成encoded\n",
    "        encoded = tokenizer.encode(txt)\n",
    "\n",
    "        #生成数据\n",
    "        for i in range(0,len(encoded)-max_length,stride):\n",
    "            input = encoded[i:i+max_length]\n",
    "            target = encoded[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input))\n",
    "            self.target_ids.append(torch.tensor(target))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index],self.target_ids[index]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2458b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9dc59818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[37605,   241, 47078,   114]]), tensor([[  241, 47078,   114, 20998]])]\n"
     ]
    }
   ],
   "source": [
    "assistant_list[0]\n",
    "dataloader = create_dataloader_v1(#raw_text 中创建一个数据加载器 但是所批次\n",
    "    assistant_list[0], batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)#数据加载器 dataloader 转换为一个迭代器\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57164d0",
   "metadata": {},
   "source": [
    "## Step8 理解embedding层工作原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08cf2fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8618, -0.6245,  0.3002],\n",
       "        [ 0.4662, -1.0248, -1.1778],\n",
       "        [ 0.1495, -0.9649,  0.6886],\n",
       "        [ 0.2623,  0.9774,  0.2025]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "本质上就是一个one-hot编码+线性层\n",
    "\"\"\"\n",
    "input_ids = [1,2,6,4]\n",
    "vocab_size = 7\n",
    "output_dim = 3\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,output_dim)\n",
    "    def forward(self,input):\n",
    "        return self.embedding(input)\n",
    "embedding = Embedding(vocab_size,output_dim)\n",
    "embedded = embedding(torch.tensor(input_ids))\n",
    "embedded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5b94a",
   "metadata": {},
   "source": [
    "## Step9 位置编码与最终输入向量的生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "afcc367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4482,  0.5212,  1.8515],\n",
       "        [-0.3122, -1.0258,  0.0761],\n",
       "        [ 0.4272, -0.3092, -0.5890],\n",
       "        [ 0.0937, -1.4066,  1.5259]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 4\n",
    "pos_embedding = torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embeded = pos_embedding(torch.arange(context_length))\n",
    "pos_embeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04327c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2075, -0.8381,  0.6974],\n",
       "        [-1.3548, -0.9634, -2.0201],\n",
       "        [-0.3824, -0.8803, -1.4132],\n",
       "        [ 0.4663, -0.6016,  1.4532]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vec = pos_embeded + embedded\n",
    "input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12e8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms4scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
