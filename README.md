# -0-GPT2-
使用GPT2-124M的模型，完成了从模型构建、模型预训练到模型微调。模型预训练结果可以做到理解中文并生成推理句子，但由于模型较小（我自己的理解，也有可能是微调数据处理的不够好），模型在问答方面的表型不尽人意。
